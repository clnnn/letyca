{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLX fine-tuning - Chart Metadata Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This notebook is based on the Microsoft official [fine-tuning cookbook](https://github.com/microsoft/Phi-3CookBook/blob/main/md/04.Fine-tuning/FineTuning_MLX.md) for Phi-3 using MLX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Install MLX-LM framework for fine-tuning LLMs using Apple Silicon GPU\n",
    "%pip install mlx-lm\n",
    "\n",
    "# Install HF datasets to load data for finetuning from Hugging Face\n",
    "%pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "We are gonna use the [clnnn/letyca-chart-metadata](https://huggingface.co/datasets/clnnn/letyca-chart-metadata) dataset from Hugging Face. The dataset is using [ShareGPT style](https://huggingface.co/datasets/philschmid/guanaco-sharegpt-style) where a data row looks like this:\n",
    "\n",
    "```json\n",
    "{\n",
    "        \"conversations\": [\n",
    "            {\n",
    "                \"from\": \"human\",\n",
    "                \"value\": \"total number of products\"\n",
    "            },\n",
    "            {\n",
    "                \"from\": \"gpt\",\n",
    "                \"value\": \"```json{\\\"chartType\\\":\\\"countLabel\\\",\\\"title\\\":\\\"Total number of products\\\"}```\"\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "```\n",
    "\n",
    "In this step we will convert the dataset into a `.jsonl` data format and the conversation will use use the specific Phi-3 prompt template:\n",
    "\n",
    "```json\n",
    "{\"text\": \"<|user|>\\ntotal number of products <|end|>\\n<|assistant|> ```json{\\\"chartType\\\":\\\"countLabel\\\",\\\"title\\\":\\\"Total number of products\\\"}``` <|end|>\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    convos = examples[\"conversations\"]\n",
    "    texts = []\n",
    "    mapper = {\"system\": \"<|system|>\", \"human\": \"<|user|>\\n\", \"gpt\": \"<|assistant|> \\n\"}\n",
    "    end_mapper = {\"system\": \"<|end|>\", \"human\": \"<|end|>\", \"gpt\": \"<|end|>\"}\n",
    "    for convo in convos:\n",
    "        text = \"\".join(f\"{mapper[(turn := x['from'])]}{x['value']} {end_mapper[turn]}\" for x in convo)\n",
    "        texts.append(f\"{text}\")\n",
    "    return {\"text\": texts}\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"clnnn/letyca-chart-metadata\", split = \"train\")\n",
    "\n",
    "# Apply the formatting function to the dataset\n",
    "dataset = dataset.map(formatting_prompts_func, batched=True, remove_columns=dataset.column_names)\n",
    "\n",
    "# Split the dataset into train and test set with 80% and 20% respectively\n",
    "train_test_split = dataset.train_test_split(test_size=0.2)\n",
    "\n",
    "# Split the test set into test and validation set with 50% each\n",
    "test_valid_split = train_test_split[\"test\"].train_test_split(test_size=0.5)\n",
    "\n",
    "# Put all the splits into a DatasetDict\n",
    "splitted_datasets = DatasetDict({\n",
    "    'train': train_test_split['train'],\n",
    "    'test': test_valid_split['test'],\n",
    "    'valid': test_valid_split['train']})\n",
    "\n",
    "# Save each split to .jsonl file in the data directory\n",
    "for split in splitted_datasets:\n",
    "    splitted_datasets[split].to_json(f\"data/{split}.jsonl\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning (default & custom configurations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_config = True\n",
    "if not custom_config: \n",
    "    !python3 -m mlx_lm.lora --model microsoft/Phi-3-mini-4k-instruct --train --data ./data --iters 1000\n",
    "else:\n",
    "    !python3 -m  mlx_lm.lora --config config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m mlx_lm.generate --model microsoft/Phi-3-mini-4k-instruct --adapter-path ./adapters --max-token 2048 --prompt \"total number of birds\" --eos-token \"<|end|>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GGUF format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "format = \"q8_0\" # f32, b16, q8_0\n",
    "\n",
    "# Fuse the model using MLX-LM\n",
    "!python3 -m mlx_lm.fuse --model microsoft/Phi-3-mini-4k-instruct --adapter-path ./adapters --de-quantize\n",
    "\n",
    "# Convert the fused model to GGUF format using llama.cpp\n",
    "!git clone https://github.com/ggerganov/llama.cpp.git && cd llama.cpp && python3 convert_hf_to_gguf.py ../lora_fused_model --outfile ../chart-metadata.gguf --outtype {format}\n",
    "\n",
    "print(\"Conversion to GGUF format is complete.\")\n",
    "!rm -rf llama.cpp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
